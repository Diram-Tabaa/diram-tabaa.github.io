---
---

@string{aps = {American Physical Society,}}

@article{tabaa2025greenhousesplatdatasetphotorealisticgreenhouse,
  abbr={ArXiv preprint},
  title={GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics}, 
  author={Diram Tabaa and Gianni Di Caro},
  journal={Submitted},
  month={Oct},
  year={2025},
  eprint={2510.01848},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  arxiv={https://arxiv.org/abs/2510.01848}, 
  preview={greenhouse-splat.gif},
  abstract={Simulating greenhouse environments is critical for developing and evaluating robotic systems for agriculture, yet existing approaches rely on simplistic or synthetic assets that limit simulation-to-real transfer. Recent advances in radiance field methods, such as Gaussian splatting, enable photorealistic reconstruction but have so far been restricted to individual plants or controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a framework and dataset for generating photorealistic greenhouse assets directly from inexpensive RGB images. The resulting assets are integrated into a ROS-based simulation with support for camera and LiDAR rendering, enabling tasks such as localization with fiducial markers. We provide a dataset of 82 cucumber plants across multiple row configurations and demonstrate its utility for robotics evaluation. GreenhouseSplat represents the first step toward greenhouse-scale radiance-field simulation and offers a foundation for future research in agricultural robotics.}
}

@article{tabaa2025fiducialmarkersplattinghighfidelity,
  abbr={ArXiv preprint},
  title={Fiducial Marker Splatting for High-Fidelity Robotics Simulations}, 
  author={Diram Tabaa and Gianni Di Caro},
  journal={Submitted},
  month={Aug},
  year={2025},
  eprint={2508.17012},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  arxiv={https://arxiv.org/abs/2508.17012},
  selected={true}, 
  preview={fiducial.png},
  abstract={High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.}
}

@article{Tabaa2024,
  abbr={thesis},
  author = "Diram Tabaa",
  title = "{SampleLapNet: A Learnable Laplacian Approach for Task-Agnostic Point Cloud Downsampling}",
  journal={Carnegie Mellon University (Senior Honors Thesis)},
  year = "2024",
  month = "9",
  url = "https://kilthub.cmu.edu/articles/thesis/SampleLapNet_A_Learnable_Laplacian_Approach_for_Task-Agnostic_Point_Cloud_Downsampling/26932201",
  doi = "10.1184/R1/26932201.v1",
  preview={samplelapnet.png},
  pdf={samplelapnet.pdf},
  selected={true},
  html={kilthub.cmu.edu/articles/thesis/SampleLapNet_A_Learnable_Laplacian_Approach_for_Task-Agnostic_Point_Cloud_Downsampling/26932201},
  abstract={Advancements in 3D sensing technologies have led to an increased reliance on point cloud data for diverse applications ranging from autonomous navigation to environmental modeling. However, the sheer volume of data collected by these technologies poses significant challenges for real-time processing and analysis. This thesis introduces SampleLapNet, a novel neural network architecture designed to address the challenges of point cloud downsampling in a task-agnostic manner. By leveraging the Laplacian operator as a geometric measure of point importance, SampleLapNet learns to predict and preserve critical geometric features during the downsampling process, thereby ensuring minimal loss of relevant information. The architecture combines the robustness of transformer models with the efficiency of Laplacian-based importance scoring to facilitate efficient preprocessing that enhances subsequent point cloud analyses. We demonstrate the effectiveness of SampleLapNet through extensive experiments on benchmark datasets, showing significant improvements in downsampling efficiency without compromising the performance of downstream tasks such as semantic segmentation. This work not only proposes a method to reduce computational demands but also provides insights into the geometric processing of 3D data, suggesting pathways for future innovations in point cloud processing.}
}

@ARTICLE{9875355,
  abbr={IEEE TIFS},
  author={Altinisik, Enes and Sencar, HÃ¼srev Taha and Tabaa, Diram},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Video Source Characterization Using Encoding and Encapsulation Characteristics}, 
  year={2022},
  volume={17},
  number={},
  pages={3211-3224},
  keywords={Encoding;Metadata;Cameras;Streaming media;Transform coding;Standards;Media;Video coding;video file containers;file metadata;model identification},
  doi={10.1109/TIFS.2022.3204210},
  google_scholar_id={u-x6o8ySG0sC},
  preview={video.png},
  abstract={We introduce the use of video coding settings for source identification and propose a new approach that incorporates encoding and encapsulation aspects of a video. To this end, a joint representation of the overall file metadata is developed and used in conjunction with a two-level hierarchical classification method. At the first level, our method groups videos into metaclasses considering several abstractions that represent high-level structural properties of file metadata. This is followed by a more nuanced classification of classes that comprise each metaclass. The method is evaluated on more than 20K videos obtained by combining four public video datasets. Tests show that a balanced accuracy of 91% is achieved in correctly identifying the class of a video among 119 video classes. This corresponds to an improvement of 6.5% over the conventional approach based on video file encapsulation characteristics. Analysis performed on a large, unlabeled video set also confirmed the aptness of our approach. To further demonstrate the versatility of encoding parameters, we consider attribution of partial video files where file metadata is not available. Our results show that, even in this limited setting that is intrinsic to forensic file recovery, an identification accuracy of 57% can be achieved through the use of a subset of encoding parameters estimated from coded video data.}
  }



